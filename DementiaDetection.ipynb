{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dementia MRI Classification (Professional Project Version)\n",
    "\n",
    "این نسخه‌ی بازنویسی‌شده شامل این بهبودهاست:\n",
    "- ساختار حرفه‌ای و پارامتری برای مسیر داده‌ها\n",
    "- ساخت دیتاست با `image_dataset_from_directory`\n",
    "- نرمال‌سازی و Data Augmentation\n",
    "- مدل CNN با `L2` و `BatchNormalization`\n",
    "- استفاده از Callbackها (`EarlyStopping`, `ReduceLROnPlateau`, `ModelCheckpoint`)\n",
    "- گزارش کامل: نمودارها، Confusion Matrix و Classification Report\n",
    "- تابع پیش‌بینی روی تصاویر جدید\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# مسیر اصلی دیتاست را اینجا تنظیم کنید.\n",
    "# ساختار مورد انتظار:\n",
    "# DATA_DIR/\n",
    "#   Non Demented/\n",
    "#   Mild Dementia/\n",
    "#   Moderate Dementia/\n",
    "#   Very mild Dementia/\n",
    "\n",
    "DATA_DIR = Path('data/train')\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 40\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'Dataset path not found: {DATA_DIR.resolve()}\\n'\n",
    "        'Please create the folder or change DATA_DIR.'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "class_names = full_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print('Classes:', class_names)\n",
    "\n",
    "full_batches = tf.data.experimental.cardinality(full_ds).numpy()\n",
    "test_batches = max(1, int(full_batches * TEST_SPLIT))\n",
    "val_batches = max(1, int(full_batches * VAL_SPLIT))\n",
    "train_batches = full_batches - test_batches - val_batches\n",
    "\n",
    "if train_batches < 1:\n",
    "    raise ValueError('Dataset is too small for current split configuration.')\n",
    "\n",
    "train_ds = full_ds.take(train_batches)\n",
    "rest_ds = full_ds.skip(train_batches)\n",
    "val_ds = rest_ds.take(val_batches)\n",
    "test_ds = rest_ds.skip(val_batches)\n",
    "\n",
    "print(f'Train batches: {tf.data.experimental.cardinality(train_ds).numpy()}')\n",
    "print(f'Val batches: {tf.data.experimental.cardinality(val_ds).numpy()}')\n",
    "print(f'Test batches: {tf.data.experimental.cardinality(test_ds).numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocessing + augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.08),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1)\n",
    "], name='data_augmentation')\n",
    "\n",
    "normalization = layers.Rescaling(1.0 / 255)\n",
    "\n",
    "def prepare(ds, training=False):\n",
    "    ds = ds.map(lambda x, y: (normalization(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = prepare(train_ds, training=True)\n",
    "val_ds = prepare(val_ds, training=False)\n",
    "test_ds = prepare(test_ds, training=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Build CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(128, 128, 3), num_classes=4):\n",
    "    l2 = regularizers.L2(1e-4)\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=l2),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=l2),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.30),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=l2),\n",
    "        layers.Dropout(0.40),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ], name='dementia_cnn')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(input_shape=(*IMG_SIZE, 3), num_classes=num_classes)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train with callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('artifacts', exist_ok=True)\n",
    "checkpoint_path = 'artifacts/best_dementia_cnn.keras'\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6),\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Learning curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(history.history['loss'], label='Train Loss')\n",
    "ax[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "ax[0].set_title('Loss Curve')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], label='Train Acc')\n",
    "ax[1].plot(history.history['val_accuracy'], label='Val Acc')\n",
    "ax[1].set_title('Accuracy Curve')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Evaluation (test set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "for x_batch, y_batch in test_ds:\n",
    "    probs = model.predict(x_batch, verbose=0)\n",
    "    y_true.extend(np.argmax(y_batch.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(probs, axis=1))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Inference on new images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, class_names, img_size=(128, 128)):\n",
    "    img = keras.utils.load_img(image_path, target_size=img_size)\n",
    "    x = keras.utils.img_to_array(img)\n",
    "    x = x / 255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    idx = int(np.argmax(probs))\n",
    "    confidence = float(probs[idx]) * 100\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{class_names[idx]} ({confidence:.2f}%)')\n",
    "    plt.show()\n",
    "\n",
    "    return class_names[idx], confidence\n",
    "\n",
    "# مثال:\n",
    "# label, conf = predict_image('data/new_cases/sample_1.jpg', model, class_names, IMG_SIZE)\n",
    "# print(label, conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Save final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = 'artifacts/dementia_cnn_final.keras'\n",
    "model.save(final_model_path)\n",
    "print('Model saved to:', final_model_path)\n",
    "print('Best checkpoint:', checkpoint_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}